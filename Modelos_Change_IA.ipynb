{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b750e0f0",
   "metadata": {},
   "source": [
    "# Datos de Voz\n",
    "\n",
    "**Para los datos de voz usamos el DataSet de \"The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS )\" obtenido en la pagina \"https://www.kaggle.com/datasets/uwrfkaggler/ravdess-emotional-speech-audio\"**\n",
    "\n",
    "Con el fin de utilizarlos de una mejor manera se obtuvieron las etiquetas de las categorias de cada audio como especifica el autor.\n",
    "\n",
    "Cada uno de los 1440 archivos tiene un nombre de archivo único. El nombre del archivo consta de un identificador numérico de 7 partes (p. ej., 03-01-06-01-02-01-12.wav). Estos identificadores definen las características del estímulo:\n",
    "\n",
    "Identificadores de nombre de archivo\n",
    "\n",
    "* Modalidad (01 = full-AV, 02 = solo video, 03 = solo audio).\n",
    "* Canal vocal (01 = habla, 02 = canción).\n",
    "* Emoción (01 = neutral, 02 = tranquilo, 03 = feliz, 04 = triste, 05 = enojado, 06 = temeroso, 07 = asco, 08 = sorprendido).\n",
    "* Intensidad emocional (01 = normal, 02 = fuerte). NOTA: No hay una intensidad fuerte para la emoción 'neutral'.\n",
    "* Declaración (01 = \"Los niños hablan junto a la puerta\", 02 = \"Los perros están sentados junto a la puerta\").\n",
    "* Repetición (01 = 1ª repetición, 02 = 2ª repetición).\n",
    "* Actor (del 01 al 24. Los actores impares son hombres, los actores pares son mujeres).\n",
    "\n",
    "En el siguiente codigo vamos a almacenar los datos en un archivo Json para poder utilizar los espectrogramas sin tener que volver a cargar los audios, usamos scipy.interpolate para tener los datos del espectrograma de un mismo tamaño.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af5930f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import librosa\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# Establecer la ruta de la carpeta que contiene los archivos de audio\n",
    "ruta_carpeta = \"archive/\"\n",
    "print(os.listdir(ruta_carpeta)[:-1])\n",
    "# Crear una lista vacía para almacenar los datos del espectrograma de cada archivo de audio\n",
    "espectrograma = []\n",
    "max_length = 240\n",
    "\n",
    "# Recorrer cada archivo de audio en la carpeta\n",
    "for carpeta in os.listdir(ruta_carpeta)[:-1]:\n",
    "    ruta_carpeta_audio = ruta_carpeta+carpeta+\"/\"\n",
    "    print(ruta_carpeta_audio)\n",
    "    for archivo_audio in os.listdir(ruta_carpeta_audio):\n",
    "        # Comprobar que el archivo es un archivo de audio\n",
    "        if archivo_audio.endswith(\".wav\"):\n",
    "            # Obtener las etiquetas de categoría del archivo de audio a partir del nombre del archivo\n",
    "            etiquetas = archivo_audio.split(\"-\")\n",
    "            Modality = etiquetas[0]  # 01-full, 02-Video, 03-audio\n",
    "            Vocal = etiquetas[1]  # 01-speech, 02-song\n",
    "            Emotion = etiquetas[2]  # 01-neutral, 02-calm, 03-happy, 04-sad, 05-angry, 06-fearful, 07-disgust, 08-surprised\n",
    "            Intensity = etiquetas[3]  # 01-normal, 02-strong\n",
    "            Statement = etiquetas[4]  # 01-\"Kids are talking by the door\", 02-\"Dogs are sitting by the door\").\n",
    "            Repetition = etiquetas[5] # 01-1st repetition, 02-2nd repetition).\n",
    "            Actor = etiquetas[6].split(\".\")[0] # Par-Mujer, Impar-Hombre\n",
    "\n",
    "            # Cargar el archivo de audio y extraer los espectrogramas\n",
    "            ruta_archivo_audio = os.path.join(ruta_carpeta_audio, archivo_audio)\n",
    "            y, sr = librosa.load(ruta_archivo_audio,sr=22050)\n",
    "            mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, fmax=8000)\n",
    "            mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "            f = interp1d(np.linspace(0, 1, mel_spec_db.shape[1]), mel_spec_db, kind='linear', axis=1)\n",
    "            mel_spec_db = f(np.linspace(0, 1, max_length))\n",
    "\n",
    "            # Agregar los datos del espectrograma a la lista junto con las etiquetas de categoría\n",
    "            espectrograma.append({\n",
    "                \"Modality\": Modality,\n",
    "                \"Vocal\": Vocal,\n",
    "                \"Emotion\": Emotion,\n",
    "                \"Intensity\": Intensity,\n",
    "                \"Statement\": Statement,\n",
    "                \"Repetition\": Repetition,\n",
    "                \"Actor\": Actor,\n",
    "                \"Espectrograma\": mel_spec_db.tolist()\n",
    "            })\n",
    "\n",
    "# Guardar los datos del espectrograma en un archivo JSON\n",
    "ruta_archivo_json = \"Espectrograma.json\"\n",
    "with open(ruta_archivo_json, \"w\") as archivo_json:\n",
    "    json.dump(espectrograma, archivo_json)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40adba47",
   "metadata": {},
   "source": [
    "# Carga de datos\n",
    "\n",
    "**Cargamos los datos y los guardamos en una variable llamada data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe3d5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "# Crear datos de entrenamiento\n",
    "ruta_archivo_json =  \"Espectrograma.json\"\n",
    "with open(ruta_archivo_json, \"r\") as archivo_json:\n",
    "    data = json.load(archivo_json)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53b54b1",
   "metadata": {},
   "source": [
    "# Datos\n",
    "\n",
    "**Guardamos los datos en un DataFrame con la finalidad de poder usarlos de una manera mas facil**\n",
    "\n",
    "Tambien hacemos un acomodo aleatorio de los datos y normalizamos los datos del espectrograma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4f6de9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.DataFrame()\n",
    "df[\"Modality\"] = [int(data[val]['Modality'])-1 for val in range(len(data)) ]\n",
    "df[\"Vocal\"] = [int(data[val]['Vocal'])-1 for val in range(len(data)) ]\n",
    "df[\"Emotion\"] = [int(data[val]['Emotion'])-1 for val in range(len(data)) ]\n",
    "df[\"Intensity\"] = [int(data[val]['Intensity'])-1 for val in range(len(data)) ]\n",
    "df[\"Statement\"] = [int(data[val]['Statement'])-1 for val in range(len(data)) ]\n",
    "df[\"Repetition\"] = [int(data[val]['Repetition'])-1 for val in range(len(data)) ]\n",
    "df[\"Actor\"] = [int(data[val]['Actor'])-1 for val in range(len(data)) ]\n",
    "df[\"Espectrograma\"] = [np.array(data[val]['Espectrograma']) for val in range(len(data)) ]\n",
    "\n",
    "df=df.sample(frac=1).reset_index(drop=True)\n",
    "print(df)\n",
    "df_normalized = df[\"Espectrograma\"].apply(lambda x: (x - x.min()) / (x.max() - x.min()) * 2 - 1)\n",
    "print(df_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cd7ae1",
   "metadata": {},
   "source": [
    "# Preparacion de datos para entrenamiento\n",
    "\n",
    "**Acomodamos los datos dejando un 20% para el test y del 80% de los datos resntante un 20% para la validacion, los datos restantes a esto seran los que usaremos para entrenar**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3adaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "porcentaje = 0.8\n",
    "total = len(df[\"Actor\"])\n",
    "split = porcentaje*total\n",
    "\n",
    "\n",
    "data_c = np.array([np.asarray(x) for x in df['Espectrograma'][:int(split)]])\n",
    "data_e = np.asarray(df[\"Actor\"][:int(split)])\n",
    "\n",
    "porcentaje = 0.8\n",
    "total = len(data_c)\n",
    "split2 = porcentaje*total\n",
    "\n",
    "#Train data \n",
    "train_data = data_c[:int(split2)]\n",
    "train_data = train_data.astype(np.float)\n",
    "train_data = train_data.reshape(-1, 128, 240, 1)\n",
    "\n",
    "train_labels = data_e[:int(split2)]\n",
    "train_labels = np.asarray(train_labels)\n",
    "train_labels = train_labels.astype(np.float)\n",
    "train_labels = train_labels.reshape(-1, 1)\n",
    "\n",
    "print(\"train_data shape: \",train_data.shape)\n",
    "print(\"train_labels shape: \",train_labels.shape)\n",
    "\n",
    "#Valid data\n",
    "val_data = data_c[int(split2):]\n",
    "val_data = val_data.astype(np.float)\n",
    "val_data = val_data.reshape(-1, 128, 240, 1)\n",
    "val_labels = data_e[int(split2):]\n",
    "val_labels = val_labels.astype(np.float)\n",
    "val_labels = val_labels.reshape(-1, 1)\n",
    "\n",
    "print(\"val_data shape: \",val_data.shape)\n",
    "print(\"val_labels shape: \",val_labels.shape)\n",
    "\n",
    "\n",
    "#Test data\n",
    "test_data = np.array([np.asarray(x) for x in df['Espectrograma'][int(split):]])\n",
    "test_data = test_data.astype(np.float)\n",
    "test_data = test_data.reshape(-1, 128, 240, 1)\n",
    "\n",
    "test_labels = np.asarray(df[\"Actor\"][int(split):])\n",
    "test_labels = test_labels.astype(np.float)\n",
    "test_labels = test_labels.reshape(-1, 1)\n",
    "\n",
    "print(\"test_data shape: \",test_data.shape)\n",
    "print(\"test_labels shape: \",test_labels.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148f1629",
   "metadata": {},
   "source": [
    "# Definimos la arquitectura para el modelo de Caracteristicas de Voz en personas\n",
    "\n",
    "**Para las caracteristicas de usuario usaremos la siguiente arquitectura**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6c65e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Definir la arquitectura de la red neuronal\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 240, 1)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), padding='same'),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), padding='same'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.08)),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.04)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(24, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33071f39",
   "metadata": {},
   "source": [
    "# Compilación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ca8c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilar el modelo\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-4),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4a4068",
   "metadata": {},
   "source": [
    "# Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f3eea5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Entrenar el modelo\n",
    "\n",
    "model.fit(train_data, \n",
    "          train_labels, \n",
    "          epochs=100, \n",
    "          steps_per_epoch = 12,\n",
    "          validation_steps = 3,\n",
    "          validation_data=(val_data, val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68f40f9",
   "metadata": {},
   "source": [
    "# Evaluación del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1152026c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar el modelo en el conjunto de prueba\n",
    "test_loss, test_acc = model.evaluate(test_data, test_labels)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab8bb5a",
   "metadata": {},
   "source": [
    "# Guardamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd405b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Personas.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e20f4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 51\n",
    "array = model.predict(test_data)[num]\n",
    "print(np.where(array == max(array)), max(array),test_labels[num])\n",
    "print(np.where(array == max(array))[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80570d8",
   "metadata": {},
   "source": [
    "# Datos para el modelo de Detección de Sentimientos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597aa389",
   "metadata": {},
   "outputs": [],
   "source": [
    "porcentaje = 0.8\n",
    "total = len(df[\"Emotion\"])\n",
    "split = porcentaje*total\n",
    "\n",
    "\n",
    "data_c = np.array([np.asarray(x) for x in df['Espectrograma'][:int(split)]])\n",
    "data_e = np.asarray(df[\"Emotion\"][:int(split)])\n",
    "\n",
    "porcentaje = 0.8\n",
    "total = len(data_c)\n",
    "split2 = porcentaje*total\n",
    "\n",
    "#Train data \n",
    "train_data = data_c[:int(split2)]\n",
    "train_data = train_data.astype(np.float)\n",
    "train_data = train_data.reshape(-1, 128, 240, 1)\n",
    "\n",
    "train_labels = data_e[:int(split2)]\n",
    "train_labels = np.asarray(train_labels)\n",
    "train_labels = train_labels.astype(np.float)\n",
    "train_labels = train_labels.reshape(-1, 1)\n",
    "\n",
    "print(\"train_data shape: \",train_data.shape)\n",
    "print(\"train_labels shape: \",train_labels.shape)\n",
    "\n",
    "#Valid data\n",
    "val_data = data_c[int(split2):]\n",
    "val_data = val_data.astype(np.float)\n",
    "val_data = val_data.reshape(-1, 128, 240, 1)\n",
    "\n",
    "val_labels = data_e[int(split2):]\n",
    "val_labels = val_labels.astype(np.float)\n",
    "val_labels = val_labels.reshape(-1, 1)\n",
    "\n",
    "print(\"val_data shape: \",val_data.shape)\n",
    "print(\"val_labels shape: \",val_labels.shape)\n",
    "\n",
    "\n",
    "#Test data\n",
    "test_data = np.array([np.asarray(x) for x in df['Espectrograma'][int(split):]])\n",
    "test_data = test_data.astype(np.float)\n",
    "test_data = test_data.reshape(-1, 128, 240, 1)\n",
    "\n",
    "test_labels = np.asarray(df[\"Emotion\"][int(split):])\n",
    "test_labels = test_labels.astype(np.float)\n",
    "test_labels = test_labels.reshape(-1, 1)\n",
    "\n",
    "print(\"test_data shape: \",test_data.shape)\n",
    "print(\"test_labels shape: \",test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7e8161",
   "metadata": {},
   "source": [
    "# Definimos la arquitectura para el modelo de Caracteristicas de emociones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264cba7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Definir la arquitectura de la red neuronal\n",
    "model_2 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 240, 1)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), padding='same'),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), padding='same'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.05)),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.02)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(8, activation='softmax')\n",
    "])\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64317c84",
   "metadata": {},
   "source": [
    "# Compilamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b41885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilar el modelo\n",
    "model_2.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-4),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fb00d6",
   "metadata": {},
   "source": [
    "# Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50a134e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar el modelo\n",
    "\n",
    "model_2.fit(train_data, \n",
    "          train_labels, \n",
    "          epochs=150, \n",
    "          steps_per_epoch = 12,\n",
    "          validation_steps = 3,\n",
    "          validation_data=(val_data, val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bede49d",
   "metadata": {},
   "source": [
    "# Evaluacion de Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4438543a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar el modelo en el conjunto de prueba\n",
    "test_loss, test_acc = model_2.evaluate(test_data, test_labels)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c41b6b",
   "metadata": {},
   "source": [
    "# Guardamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a737b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('emociones_Spec.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
